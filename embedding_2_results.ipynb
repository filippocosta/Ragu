{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n",
      "(944238, 2)\n",
      "Finished 20000 entries\n",
      "Finished 40000 entries\n",
      "Finished 60000 entries\n",
      "Kmer:  10\n",
      "944238\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 2)           1888476   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 2)                 32        \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                30        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 11        \n",
      "=================================================================\n",
      "Total params: 1,888,549\n",
      "Trainable params: 1,888,549\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 469529 samples, validate on 117383 samples\n",
      "Epoch 1/5\n",
      "469529/469529 - 603s - loss: 0.3677 - accuracy: 0.8275 - val_loss: 0.2275 - val_accuracy: 0.9100\n",
      "Epoch 2/5\n",
      "469529/469529 - 576s - loss: 0.1701 - accuracy: 0.9366 - val_loss: 0.2023 - val_accuracy: 0.9219\n",
      "Epoch 3/5\n",
      "469529/469529 - 592s - loss: 0.1402 - accuracy: 0.9485 - val_loss: 0.1880 - val_accuracy: 0.9291\n",
      "Epoch 4/5\n",
      "469529/469529 - 584s - loss: 0.1126 - accuracy: 0.9590 - val_loss: 0.1501 - val_accuracy: 0.9450\n",
      "Epoch 5/5\n",
      "469529/469529 - 566s - loss: 0.0753 - accuracy: 0.9733 - val_loss: 0.1159 - val_accuracy: 0.9575\n",
      "Accuracy: 95.79%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import subprocess\n",
    "import numpy as np\n",
    "from Bio import SeqIO\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import SGD, Adam, Adadelta, RMSprop\n",
    "from tensorflow.keras.layers import Conv1D, Dense, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, LSTM, SimpleRNN, GRU\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n",
    "\n",
    "print(emb_weights.shape)\n",
    "intr_file = '../ubuntu/data/hg19_intr_clean.fa'\n",
    "depl_file = '../ubuntu/data/hg19_depl_clean.fa'\n",
    "e = 0\n",
    "intr_seqs = []\n",
    "depl_seqs = []\n",
    "for intr, depl in zip(SeqIO.parse(intr_file, 'fasta'), SeqIO.parse(depl_file, 'fasta')):\n",
    "    \n",
    "    step = 200; jump = 1; a = 0; b = step; n_jumps = 5\n",
    "    for j in range(n_jumps):\n",
    "        s_intr = str(intr.seq)[a:b]\n",
    "        s_depl = str(depl.seq)[a:b]\n",
    "        intr_seqs.append(s_intr)\n",
    "        depl_seqs.append(s_depl)\n",
    "        a = a + jump\n",
    "        b = a + step\n",
    "    \n",
    "    e = e + 1\n",
    "    if e%20000 == 0:\n",
    "        print('Finished ' + str(e) + ' entries')\n",
    "        \n",
    "def getKmers(sequence, size):\n",
    "    return [sequence[x:x+size].upper() for x in range(len(sequence) - size + 1)]\n",
    "\n",
    "kmer = [10]\n",
    "d = 2\n",
    "results = []\n",
    "emb = []\n",
    "for k in kmer:\n",
    "    print('Kmer: ',k)\n",
    "    intr_texts = [' '.join(getKmers(i, k)) for i in intr_seqs]\n",
    "    depl_texts = [' '.join(getKmers(i, k)) for i in depl_seqs]\n",
    "    merge_texts = intr_texts + depl_texts\n",
    "    labels = list(np.ones(len(intr_texts))) + list(np.zeros(len(depl_texts)))\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(merge_texts)\n",
    "\n",
    "    encoded_docs = tokenizer.texts_to_sequences(merge_texts)\n",
    "    max_length = max([len(s.split()) for s in merge_texts])\n",
    "    X = pad_sequences(encoded_docs, maxlen = max_length, padding = 'post')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size = 0.20, random_state = 42)\n",
    "\n",
    "    y_train = np.asarray(y_train)\n",
    "    y_test = np.asarray(y_test)\n",
    "\n",
    "    max_length = max([len(s.split()) for s in merge_texts])\n",
    "\n",
    "    vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "    from tensorflow.keras.layers import Embedding, GlobalAveragePooling1D, LSTM, SimpleRNN, GRU, Bidirectional\n",
    "    print(vocab_size)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, d))\n",
    "    model.add(Bidirectional(LSTM(int(d/2))))\n",
    "    model.add(Dense(10, activation = 'relu'))\n",
    "    model.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "    epochs = 5\n",
    "     \n",
    "    model.compile(loss = 'binary_crossentropy', optimizer = 'rmsprop', metrics = ['accuracy'])\n",
    "    # checkpoint = ModelCheckpoint(\"weights.best.hdf5\", monitor = 'val_acc', verbose = 1, \n",
    "    #                             save_best_only = True, mode = 'max')\n",
    "            \n",
    "    print(model.summary())\n",
    "\n",
    "    history = model.fit(X_train, y_train, \n",
    "                        epochs = epochs, verbose = 2, validation_split = 0.2, batch_size = 32, shuffle = True, \n",
    "    #                    callbacks = [checkpoint]\n",
    "                       )\n",
    "\n",
    "    predicted_labels = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, [np.round(i[0]) for i in predicted_labels])\n",
    "    cm = cm.astype('float') / cm.sum(axis = 1)[:, np.newaxis]\n",
    "\n",
    "    scores = model.evaluate(X_test, y_test, verbose = 0)\n",
    "    print(\"Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "    results.append(scores[1]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
